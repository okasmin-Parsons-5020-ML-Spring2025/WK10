{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 10\n",
        "\n",
        "Text Processing and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
        "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/text_utils.py\n",
        "!wget -qO- https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/datasets/text/movie_reviews.tar.gz | tar xz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### All Scikit-Learn Now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import NMF, TruncatedSVD, PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from data_utils import MinMaxScaler\n",
        "from data_utils import display_confusion_matrix\n",
        "\n",
        "from text_utils import get_top_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Classification\n",
        "\n",
        "Let's try to predict whether a given review expresses positive or negative feelings towards a movie.\n",
        "\n",
        "We have a dataset that basically has $2$ features per record: `review` and `sentiment`.\n",
        "\n",
        "Let's load and look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_df = pd.read_csv(\"./data/text/movie_reviews.csv\")\n",
        "reviews_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Features\n",
        "\n",
        "Text is a very different kind of feature...\n",
        "\n",
        "We do want to turn it into numbers somehow in order to apply some of the methods and models we've been learning about, but how to do that exactly is not entirely obvious.\n",
        "\n",
        "We can try to extract some numerical information about the review text. Maybe something like the length of the review or the relative amount of punctuation marks or digits can be indicative of its sentiment.\n",
        "\n",
        "Let's define some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_characters(st):\n",
        "  return len(\"\".join(st.split()))\n",
        "\n",
        "def count_words(st):\n",
        "  return len(st.split(\" \"))\n",
        "\n",
        "def count_punctuation(st):\n",
        "  return len([c for c in st if c in string.punctuation])\n",
        "\n",
        "def count_digits(st):\n",
        "  return len([c for c in st if c in string.digits])\n",
        "\n",
        "def get_punctuation_pct(st):\n",
        "  return count_punctuation(st) / count_characters(st)\n",
        "\n",
        "def get_digit_pct(st):\n",
        "  return count_digits(st) / count_characters(st)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's apply some of these to our `DataFrame` to create numerical features that we can eventually use in a classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_df[\"char_count\"] = reviews_df[\"review\"].apply(count_characters)\n",
        "reviews_df[\"word_count\"] = reviews_df[\"review\"].apply(count_words)\n",
        "reviews_df[\"punctuation_pct\"] = reviews_df[\"review\"].apply(get_punctuation_pct)\n",
        "reviews_df[\"digit_pct\"] = reviews_df[\"review\"].apply(get_digit_pct)\n",
        "\n",
        "reviews_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we model this data, let's look at some of these features and see if we can visually identify the negative and positive reviews on plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(reviews_df[\"word_count\"], reviews_df[\"punctuation_pct\"], c=reviews_df[\"sentiment\"])\n",
        "plt.title(\"Punctuation % x Word Count\")\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(reviews_df[\"digit_pct\"], reviews_df[\"punctuation_pct\"], c=reviews_df[\"sentiment\"])\n",
        "plt.title(\"Digit % x Word Count\")\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(reviews_df[\"word_count\"], reviews_df[\"char_count\"], c=reviews_df[\"sentiment\"])\n",
        "plt.title(\"Character Count x Word Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is not very promising. It doesn't seem like these features contain enough information to help us extract meaning from the reviews.\n",
        "\n",
        "Let's just confirm this suspicion by creating a classifier.\n",
        "\n",
        "We'll use a `MinMaxScaler` since some of the features are already in a $[0,1]$ range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mScaler = MinMaxScaler()\n",
        "\n",
        "simple_feats_df = reviews_df.drop(columns=[\"review\", \"sentiment\"])\n",
        "simple_feats_scaled_df = mScaler.fit_transform(simple_feats_df)\n",
        "\n",
        "simple_feats_scaled_df[\"sentiment\"] = reviews_df[\"sentiment\"]\n",
        "\n",
        "simple_feats_scaled_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train/Test splitting should've been done before scaling, but this is just a quick experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_train_df, reviews_test_df = train_test_split(simple_feats_scaled_df, test_size=0.2)\n",
        "\n",
        "reviews_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mClassifier = RandomForestClassifier()\n",
        "\n",
        "train_feats = reviews_train_df.drop(columns=[\"sentiment\"])\n",
        "train_labels = reviews_train_df[\"sentiment\"]\n",
        "\n",
        "mClassifier.fit(train_feats, train_labels)\n",
        "\n",
        "train_preds = mClassifier.predict(train_feats)\n",
        "\n",
        "accuracy_score(train_labels, train_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_feats = reviews_test_df.drop(columns=[\"sentiment\"])\n",
        "test_labels = reviews_test_df[\"sentiment\"]\n",
        "\n",
        "test_preds = mClassifier.predict(test_feats)\n",
        "\n",
        "accuracy_score(test_labels, test_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤”\n",
        "\n",
        "Our model is just as good as a random guess.\n",
        "\n",
        "We'll have to use something else.\n",
        "\n",
        "Back to the drawing board."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag-of-Words (BoW)\n",
        "\n",
        "This is a way of modeling sentences as a function of their words. We can think of it as a specialized version of One-Hot Encoding, where we turn our single-column `review` feature into a series of numbers that represent which words are present in that review. If a word is not present, that column gets a $0$, if the word is present, the column gets an integer that represents the total number of times that word was used in the review.\n",
        "\n",
        "There are some specificities to keep in mind when we encode text this way. We need to figure out what constitutes a _word_ and what kind of words we want to ignore.\n",
        "\n",
        "Do we consider the words `type`, `types`, `typed` as the same word or $3$ different words ?\n",
        "\n",
        "Do we consider words like `a`, `the`, `of`, `in`, etc ... in our encoding ? What other kinds of words should be treated differently ?\n",
        "\n",
        "The first consideration is part of the process of _tokenization_, or, how we turn sequences (of words) into its constitutive components (tokens). There are libraries and pre-trained models that can help us with that task.\n",
        "\n",
        "To answer part of the second question: it's best to remove common words from text before processing it because they don't add meaning or variance to our data. These words are commonly referred to as _stop words_, or _negative dictionary_, and, again, we can find lists of common _stop words_ for different languages in text-processing libraries and packages.\n",
        "\n",
        "<img src=\"./imgs/tokens-00.jpg\" width=\"720px\" />\n",
        "\n",
        "Our dataset can have other words that show up very frequently, but aren't generally considered _stop words_. For example, a dataset about movie reviews might have the words `movie`, `good`, `director`, etc in every single review. While not typical _stop words_, they should be ignored during encoding because they add no meaningful differentiation to our data.\n",
        "\n",
        "The same is true for words that are rare and only show up in a small fraction of our sentences/reviews.\n",
        "\n",
        "This process of encoding text sequences by the count of their words is called Vectorization. This method of encoding keeps track of which words are present in a series of words, and how common they are, but without any significant information about the order of the words or where they occurred in the original text.\n",
        "\n",
        "That's why models created this way are called _Bag-of-Word_ models: they model _what_ words are there, but not _where_ they occurred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorize by Count\n",
        "\n",
        "Let's use the `Scikit-Learn` class `CountVectorizer` to encode our reviews.\n",
        "\n",
        "The `min_df` and `max_df` parameters to the class constructor determine the minimum and maximum document frequencies to consider when encoding our data.\n",
        "\n",
        "With `min_df=5` and `max_df=0.75`, the vectorizer ignores words that show up in less than $5$ reviews and words that show up in more than $75\\%$ of reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_df = pd.read_csv(\"./data/text/movie_reviews.csv\")\n",
        "\n",
        "reviews_train_df, reviews_test_df = train_test_split(reviews_df, test_size=0.2, random_state=1010)\n",
        "reviews_train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# english stop words -- the, a, etc.\n",
        "# min_df - minimum document frequency (ie if word shows up less than 5 times then ignore it) - too specific\n",
        "# max_df - maximum document frequency (ie if word shows up in more than 3/4 documents then ignore it) - too common\n",
        "# max_features - limit number of features for performance, optimization (ie only keep 30,000 columns that have most items)\n",
        "mCV = CountVectorizer(stop_words=\"english\", min_df=5, max_df=0.75, max_features=30_000)\n",
        "\n",
        "reviews_train_vct = mCV.fit_transform(reviews_train_df[\"review\"])\n",
        "reviews_test_vct = mCV.transform(reviews_test_df[\"review\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we print our encoded features, we should see something like this:\n",
        "\n",
        "<img src=\"./imgs/vector-00.jpg\" width=\"720px\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_train_vct\n",
        "\n",
        "# see that have 20,000 rows for all 20,000 reviews\n",
        "# have 24,066 features \n",
        "# but not actually storing 20,000 x 24,066 because many entries are empty "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# can turn Compressed Sparse Row sparse matrix into array, but is extremely big\n",
        "reviews_train_vct.toarray()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But we don't.\n",
        "\n",
        "What !?\n",
        "\n",
        "### Sparse Matrices\n",
        "\n",
        "This is why we have to move beyond `DataFrames` for text encoding.\n",
        "\n",
        "We have thousands of reviews and thousands of possible words in our vocabulary. Encoding this information using a `DataFrame` would be extremely inefficient and wasteful because most of the columns for any given row is most likely a $0$. Even if a review used $1\\text{,}000$ unique words, that would still mean that only about $10\\%%$ of our columns would have non-zero values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Sparse Matrices\n",
        "\n",
        "The `CountVectorizer` object has functions that give us information about the words it encountered.\n",
        "\n",
        "`get_feature_names_out()`: returns a list of the words seen in the dataset and encoded.\n",
        "\n",
        "`inverse_transform()`: can be used to turn a sequence of word counts back into actual words, but without the order information of the original sentence.\n",
        "\n",
        "And, we can index into our vector of reviews with `[]` to get a specific review. These are encoded as sparse matrices, so we have to do a bit of work to get to the actual words and their counts:\n",
        "\n",
        "- It helps to use the `nonzero()` function to get a list of the indices of words that are actually present in that review.\n",
        "\n",
        "- Once we have the indices, we can use them to access the review vector, and get the non-zero word counts from specific locations in the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = mCV.get_feature_names_out()\n",
        "\n",
        "print(len(vocab))\n",
        "display(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get indices of non-zero counts for words in the first review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_train_vct[0].nonzero()\n",
        "\n",
        "# reviews_train_vct[:2].nonzero()\n",
        "\n",
        "# result is 2 lists\n",
        "# first list is the rows the nonzero elements are in\n",
        "# second are the nonzero columns\n",
        "\n",
        "# if only want to see the columns of nonzero since we're only getting 1 row anyway...\n",
        "_, feature_idxs = reviews_train_vct[0].nonzero()\n",
        "print(feature_idxs)\n",
        "\n",
        "# now lets see what words are at each nonzero idx\n",
        "vocab[feature_idxs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get counts from those indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_train_vct[reviews_train_vct[0].nonzero()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get words in a review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mCV.inverse_transform(reviews_train_vct[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, using the non-zero indices to index into the `vocab` list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab[reviews_train_vct[0].nonzero()[1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use these functions to order the words of a review by frequency.\n",
        "\n",
        "The process is:\n",
        "\n",
        "- Get a `review` by indexing into our list of encoded reviews\n",
        "- Count the number of tokens/words in the review\n",
        "- Use [argsort()](https://numpy.org/doc/2.1/reference/generated/numpy.argsort.html) to get the order of indices that would sort the word counts\n",
        "  - Use negative counts to get the counts ordered from largest to smallest\n",
        "- Use the first `word_count` items of this array to index into our vocab and get the actual words of the review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "review = reviews_train_vct[0]\n",
        "\n",
        "word_count = len(review.nonzero()[0])\n",
        "\n",
        "sorted_idxs = (-review.toarray()[0]).argsort()\n",
        "\n",
        "vocab[sorted_idxs[:word_count]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This seems like a useful enough operation, that maybe it should be a function that we can use on any sparse matrix of frequency counts...\n",
        "\n",
        "The `get_top_words(cnt, vocab, n)` function in `text_utils` will return the top `n` words of `cnt`, a count vector or count matrix (list of vectors).\n",
        "\n",
        "Omitting `n` makes the function return all of the words present in the sequences, ordered by frequency.\n",
        "\n",
        "The returned value is a tuple of words and their counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from text_utils import get_top_words\n",
        "\n",
        "words, counts = get_top_words(reviews_train_vct[0], vocab)\n",
        "\n",
        "display(words)\n",
        "display(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classifying by Count\n",
        "\n",
        "Ok. After that little bit of a detour to explore vector count sparse matrices, we are back to our classification problem.\n",
        "\n",
        "Seems like we should be able to classify whether a review is positive or negative by looking at the words used...\n",
        "\n",
        "Let's train a `RandomForestClassifier` and validate with our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mClassifier = RandomForestClassifier()\n",
        "\n",
        "train_labels = reviews_train_df[\"sentiment\"]\n",
        "\n",
        "mClassifier.fit(reviews_train_vct, train_labels)\n",
        "\n",
        "train_preds = mClassifier.predict(reviews_train_vct)\n",
        "\n",
        "accuracy_score(train_labels, train_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not bad. Promising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_labels = reviews_test_df[\"sentiment\"]\n",
        "\n",
        "test_preds = mClassifier.predict(reviews_test_vct)\n",
        "\n",
        "accuracy_score(test_labels, test_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok! This is not bad.\n",
        "\n",
        "After learning about count vectorization and sparse matrices, the code for doing this is actually quite simple.\n",
        "\n",
        "We could adjust parameters of the classifier or the vectorizer to improve this, but using a `RandomForestClassifier` for this task is quite inefficient.\n",
        "\n",
        "Let's look at a different kind of classifier before we continue exploring vectorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naive Bayes\n",
        "\n",
        "Bayesian statistics is a complete and complex field of math and philosophy. At a high level, it's a theory that allows for probabilities (of events, measurements, classifications, etc) to be updated based on the presence of (new) data.\n",
        "\n",
        "We are going to look at a very slim portion of Bayesian statistics to get a basic understanding of how this theory can be applied within Machine Learning algorithms.\n",
        "\n",
        "The Naive Bayes methods are a set of supervised learning algorithms based on a version of Bayes' theorem that assumes that all of our features are independent.\n",
        "\n",
        "As applied to a classification problem, this theorem has the following form:\n",
        "\n",
        "$$P\\left(y \\middle| x_1, x_2, \\ldots, x_n\\right) = \\frac{P\\left(y\\right)P\\left(x_1, x_2, \\ldots, x_n \\middle| y \\right)}{P\\left(x_1, x_2, \\ldots, x_n\\right)}$$\n",
        "\n",
        "This is an eyeful, but given that $y$ is the class associated with feature measurements $x_1, x_2, \\ldots, x_n$, it reads:\n",
        "\n",
        "The probability that a given set of measurements ($x_1, x_2, \\ldots, x_n$) represents an object of class $y$ is equal to the probability of seeing an object of class $y$ in our dataset, multiplied by the probability that an object of class $y$ has measurements $x_1, x_2, \\ldots, x_n$, divided by how common that particular set of measurements are.\n",
        "\n",
        "$P\\left(y\\right)$ is calculated by measuring how many items of our dataset represent an object of class $y$. If we have $10$ objects that are $y$ in a dataset of $50$ objects, our $P\\left(y\\right) = \\frac{10}{50}$.\n",
        "\n",
        "Likewise, $P\\left(x_1, x_2, \\ldots, x_n\\right)$ represents how many times this exact combination of measurements showed up in our dataset. If only one row out of $50$ has this combination of input features, then $P\\left(x_1, x_2, \\ldots, x_n\\right) = \\frac{1}{50}$.\n",
        "\n",
        "$P\\left(x_1, x_2, \\ldots, x_n \\middle| y \\right)$ is the trickier bit, but it gets simplified by the _naive_ assumption of feature independence and can be split into multiple terms:\n",
        "\n",
        "$P\\left(x_1 \\middle| y \\right) \\cdot P\\left(x_2 \\middle| y \\right) \\cdot\\ldots\\cdot P\\left(x_n \\middle| y \\right)$\n",
        "\n",
        "These are the probabilities that items of class $y$ have specific values for $x_1, x_2, \\ldots x_n$. For example, if in our dataset of $50$ elements, $10$ have class $y$, and $2$ out of those $10$ have a particular value $x_1$ for the first feature, $P\\left(x_1 \\middle| y \\right) = \\frac{2}{10}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Naive Bayes Text Example\n",
        "\n",
        "Let's pretend we want to calculate the probability that a review with the words `awful`, `bloody`, `guns` and `park` is **negative**.\n",
        "\n",
        "This is equivalent to calculating:\n",
        "$$P\\left(negative \\middle| \\text{awful}, \\text{bloody}, \\text{guns}, \\text{park} \\right) = \\frac{P\\left(negative\\right) P\\left(\\text{awful}, \\text{bloody}, \\text{guns}, \\text{park} \\middle| negative \\right)}{P\\left(\\text{awful}, \\text{bloody}, \\text{guns}, \\text{park}\\right)}$$\n",
        "\n",
        "$P\\left(\\text{negative}\\right)$ is equal to the proportion of **negative** reviews in the dataset. If half are positive and half are negative, $P\\left(\\text{negative}\\right) = 0.5$.\n",
        "\n",
        "$P\\left(\\text{awful}, \\text{bloody}, \\text{guns}, \\text{park}\\right)$ is the proportion of the number of reviews in the dataset that have all four words `awful`, `bloody`, `guns` and `park`.\n",
        "\n",
        "$P\\left(\\text{awful}, \\text{bloody}, \\text{guns}, \\text{park} \\middle| negative \\right)$ can be simplified to $P\\left(\\text{awful} \\middle| negative \\right) \\cdot P\\left(\\text{bloody} \\middle| negative \\right) \\cdot P\\left(\\text{guns} \\middle| negative \\right) \\cdot P\\left(\\text{park} \\middle| negative \\right)$\n",
        "\n",
        "$P\\left(\\text{awful} \\middle| negative \\right)$ is the proportion of negative reviews that have the word `awful`, $P\\left(\\text{bloody} \\middle| negative \\right)$ is the proportion of negative review with `bloody` in it, etc etc etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why ????\n",
        "\n",
        "It might not be obvious at first, but when used for classification of datasets with sparse feature vectors, the process described above can be extremely efficient because _fitting_ the model means calculating a few probability constants from our training dataset. All of the $P()$ terms on the right hand side of the Bayes equation are basic proportions calculated with addition and division operations.\n",
        "\n",
        "`Scikit-Learn` has different flavors of Naive Bayes classifiers that make further assumptions about the distributions of the input features and how $P\\left(X \\middle| y \\right)$ can be simplified.\n",
        "\n",
        "- [Gaussian](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) Naive Bayes assumes the features have gaussian distributions. This is good for datasets with continuous-valued inputs.\n",
        "- [Bernoulli](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) Naive Bayes assumes the features are all binary values (One-Hot Encoding).\n",
        "- [Categorical](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) Naive Bayes assumes our features are integers that represent categories (Ordinal Encoding).\n",
        "- [Multinomial](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) Naive Bayes assumes our features are discrete measurements.\n",
        "\n",
        "Given that the feature vectors computed with `CountVectorizer` represent word counts, it makes sense for us to use a Multinomial classifier for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: repeat classification using the appropriate Bayes model\n",
        "\n",
        "mClassifierB = MultinomialNB()\n",
        "\n",
        "# train_labels = reviews_train_df[\"sentiment\"]\n",
        "\n",
        "mClassifierB.fit(reviews_train_vct, train_labels)\n",
        "\n",
        "train_preds_B = mClassifierB.predict(reviews_train_vct)\n",
        "\n",
        "accuracy_score(train_labels, train_preds_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### N-Grams\n",
        "\n",
        "Now that we have an efficient classifier for sparse count feature vectors we can finally experiment with n-grams.\n",
        "\n",
        "In its simplest form, the Bag-of-Words method doesn't take into consideration any information about the order or location of the words in a sequence of words. We can, however, set it up to count pairs (or triplets, or quadruplets, etc) of words instead of single words.\n",
        "\n",
        "So, instead of breaking up \"_it was a good movie_\", like this:\n",
        "|it|was|a|good|movie|\n",
        "|-|-|-|-|-|\n",
        "\n",
        "It breaks it up like this:\n",
        "\n",
        "|it was|was a|a good|good movie|\n",
        "|-|-|-|-|\n",
        "\n",
        "These are the 2-grams (or bi-grams) of our sentence, but the concept can be extended to any integer value of $n$ to extract counts for different lengths of n-grams.\n",
        "\n",
        "While this doesn't help with location information, it does extract some information about word order and common phrases.\n",
        "\n",
        "To extract n-grams during vectorization, we can give `CountVectorizer` a range of values to consider with the parameter `ngram_range`. A value of $(2,2)$ will only extract bigrams, while $(1,2)$ will extract counts for single words and pairs of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mCV = CountVectorizer(stop_words=\"english\", min_df=5, max_df=0.75, max_features=50_000, ngram_range=(2, 2))\n",
        "\n",
        "reviews_train_vct = mCV.fit_transform(reviews_train_df[\"review\"])\n",
        "reviews_test_vct = mCV.transform(reviews_test_df[\"review\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `CountVectorizer` functions we saw above and our `get_top_words()` function will work the same way. The only difference is that right now our features represent counts for pairs of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = mCV.get_feature_names_out()\n",
        "print(len(vocab))\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mCV.inverse_transform(reviews_train_vct[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_top_words(reviews_train_vct[0], vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and Validate\n",
        "\n",
        "Let's try it out !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mClassifier = MultinomialNB()\n",
        "\n",
        "train_labels = reviews_train_df[\"sentiment\"]\n",
        "\n",
        "mClassifier.fit(reviews_train_vct, train_labels)\n",
        "\n",
        "train_preds = mClassifier.predict(reviews_train_vct)\n",
        "\n",
        "accuracy_score(train_labels, train_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_labels = reviews_test_df[\"sentiment\"]\n",
        "\n",
        "test_preds = mClassifier.predict(reviews_test_vct)\n",
        "\n",
        "accuracy_score(test_labels, test_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TF-IDF\n",
        "\n",
        "Another way to vectorize our reviews into a Bag-of-Words is to use a slightly smarter and more specific way of counting words in our reviews.\n",
        "\n",
        "Term Frequency-Inverse Document Frequency is a technique used to \"count\" words and scale the counts by how important a word might be to a document/review.\n",
        "\n",
        "It does this by calculating two values for each tokenized word in a review:\n",
        "- _**Term Frequency**_: the relative frequency of the word within a document/review.\n",
        "- _**Document Frequency**_: the relative frequency of the number of documents in the dataset that have this word. It measures how much information a word carries by calculating how rare it is. What gets used in the actual `tf-idf` calculation is the $log()$ of the inverse of this value.\n",
        "\n",
        "In math, this is:\n",
        "\n",
        "$$ tf(t, d) = \\frac{Count(t)}{| d |} \\hspace{20pt}\\hspace{20pt} idf(t, D) = log\\left(\\frac{| D |}{Count(d : t \\in d)}\\right)$$\n",
        "\n",
        "Where:\n",
        "- $Count(t)$ is the number of times a word appears in a document.\n",
        "- $| d |$ is the length of the document, in words.\n",
        "- $| D |$ is the total number of documents in the dataset.\n",
        "- $Count(d : t \\in d)$ is the number of documents in the dataset that have the word $t$ in them.\n",
        "\n",
        "Of course `cikit-Learn` has a builtin tf-idf Vectorizer that will do this for us. We instantiate it just like the `CountVectorizer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mTfidV = TfidfVectorizer(stop_words=\"english\", min_df=5, max_df=0.75, max_features=50_000, ngram_range=(1, 1))\n",
        "\n",
        "reviews_train_vct = mTfidV.fit_transform(reviews_train_df[\"review\"])\n",
        "reviews_test_vct = mTfidV.transform(reviews_test_df[\"review\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `TfidfVectorizer` has all the functions we saw above in the `CountVectorizer` object, and our `get_top_words()` function will work the same way with our tf-idf vectors. The difference is that now our features are not plain integer counts of words or n-grams, but our tf-idf importance metric. The higher the metric, the more significant the word (or n-gram) within our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = mTfidV.get_feature_names_out()\n",
        "print(len(vocab))\n",
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mTfidV.inverse_transform(reviews_train_vct[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_top_words(reviews_train_vct[0], vocab, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification with tf-idf\n",
        "\n",
        "This stays the same.\n",
        "\n",
        "While the multinomial classifier normally requires integer features, in practice, fractional counts such as the ones computed with a `TfidfVectorizer` also work.\n",
        "\n",
        "We could turn these into `int`s by multiplying them by $100$... but we don't have to. A `MultinomialNB` is still the best option because our td-idf values represent a kind of count. They aren't continuous, unbounded, `float` values, so it wouldn't make sense to use a Gaussian Bayes classifier, for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mClassifier = MultinomialNB()\n",
        "\n",
        "train_labels = reviews_train_df[\"sentiment\"]\n",
        "\n",
        "mClassifier.fit(reviews_train_vct, train_labels)\n",
        "\n",
        "train_preds = mClassifier.predict(reviews_train_vct)\n",
        "\n",
        "accuracy_score(train_labels, train_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_labels = reviews_test_df[\"sentiment\"]\n",
        "\n",
        "test_preds = mClassifier.predict(reviews_test_vct)\n",
        "\n",
        "accuracy_score(test_labels, test_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not bad.\n",
        "\n",
        "How does the choice of n-gram range affect classification by tf-idf ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Evaluate the effect of n-grams in the TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "Our movie reviews dataset is rich in information. It contains details and descriptions of movies, actors, directors, etc, along with other patterns and trends that weren't directly used while training our binary sentiment classifier. This is probably true of most natural language text datasets, and probably has to due with the nature of languages and how they evolved to have structure and carry dense amounts of information... Unlike a pixel, a single word by itself will mean something, even if ambiguously.\n",
        "\n",
        "What this means is, there are usually other patterns and trends, that are independent of outcome variables, that we can try to extract from datasets like this.\n",
        "\n",
        "How do we extract information when we don't have \"answers\" in our dataset ? Unsupervised Learning! And in this case we'll start by looking at Clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering\n",
        "\n",
        "Just like we clustered numerical data and pixels by finding locations in our feature space to represent, or capture, sections of our dataset, we can imagine finding specific locations in our feature space to represent sub-sets of our reviews.\n",
        "\n",
        "When we clustered pixels, our original features were `R`, `G`, `B` channel values, and so our cluster centers could be considered a set of representative colors for our image.\n",
        "\n",
        "When we clustered wines, the cluster centers had the same $10$ features as our original dataset, and represented meaningful characteristics for the wines in each clusters.\n",
        "\n",
        "For reviews ... our dataset has $10\\text{,}000$ to $50\\text{,}000$ features after we vectorize our reviews. Even if most of these values are $0$, we still have a dataset with $50\\text{,}000$ features. If we cluster based on these features, the resulting cluster centers will have the same features and values with the same meaning as the original data.\n",
        "\n",
        "This means that instead of being representative colors or wine characteristics, our cluster centers will be sets of words that represent a subsection of out dataset.\n",
        "\n",
        "Let's take a look. We'll start by clustering our reviews into $8$ groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_df = pd.read_csv(\"./data/text/movie_reviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mVec = TfidfVectorizer(stop_words=\"english\", min_df=5, max_df=0.5, max_features=50_000, ngram_range=(1, 1))\n",
        "\n",
        "reviews_vct = mVec.fit_transform(reviews_df[\"review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = mVec.get_feature_names_out()\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mClust = KMeans(n_clusters=8, random_state=800)\n",
        "reviews_km = mClust.fit_predict(reviews_vct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check our cluster centers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mClust.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤”\n",
        "Maybe we can check the shape of these... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mClust.cluster_centers_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok. That makes sense. We have $8$ clusters with about $24\\text{,}000$ features each.\n",
        "\n",
        "We can think of these as the list of words that would've been in the $8$ most representative reviews of our dataset.\n",
        "\n",
        "We can \"unpack\" them using our `get_top_words()` function. The cluster centers are the reviews, the `TfidfVectorizer` object has our vocabulary and we can look at the top $8$ - $10$ words in each review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_top_words(mClust.cluster_centers_, vocab, 8)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "There's something here.\n",
        "\n",
        "Some of the cluster center words seem to be indicative of the type of movies in those cluster, or even whether they are TV series.\n",
        "\n",
        "Can we improve the legibility of our clusters ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Experiment with CountVectorizer, the TfidfVectorizer parameters and/or N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decomposition\n",
        "\n",
        "We're clustering over $20\\text{,}000$ - $50\\text{,}000$ features of very sparse data. KMeans clustering and other algorithms might benefit from a reduction in the number of features that they have to consider.\n",
        "\n",
        "Last week we saw how to do something like this with `PCA`. `PCA` is the MVP of all decomposition algorithms, and we could use `PCA` here, but given our type of data, we should use something a little more specific.\n",
        "\n",
        "If we read the documentation for `PCA` it will mention something about how it _centers the data_ before calculating the decomposition, meaning that it shifts all of the input features so their average is $0$. This is fine for continuous features, and in most of those cases our data will already have been shifted and normalized using something like `StandardScaler` by the time it gets to `PCA`.\n",
        "\n",
        "But, it doesn't make sense to do that with our text features here due to the scale and sparseness of our data. First, it is very inefficient to go through the process of calculating averages and shifting these features. Second, even if we are using tf-idf values that aren't whole numbers, they don't really represent continuous values of a distribution; they're more like counters that use floating point values, so calculating these averages can introduce unwanted distortions to our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Singular Value Decomposition\n",
        "\n",
        "We can use a more general form of the `PCA` algorithm called _Singular Value Decomposition_ that, like `PCA`, will decompose our dataset into smaller datasets of combined features ordered by importance, but unlike `PCA` it doesn't have to center our data before processing.\n",
        "\n",
        "The computation for doing `SVD` and `PCA` decomposition is the same, but due to the centering of the data, `PCA` can take some shortcuts.\n",
        "\n",
        "If we can think of `PCA` decomposition as something that refactors our dataset into two dense matrices like this, where the first one holds our new features and can be _abbreviated_ by selecting columns with the greatest amount of combined variance:\n",
        "\n",
        "<img src=\"./imgs/pca-01.jpg\" width=\"720px\" />\n",
        "\n",
        "Singular Value Decomposition does something like this:\n",
        "\n",
        "<img src=\"./imgs/svd-01.jpg\" width=\"720px\" />\n",
        "\n",
        "It refactors our data into $3$ matrices, where one of them only has elements on the diagonal. We get our transformed dataset by multiplying the first matrix by a _truncated_/_abbreviated_ version of this diagonal matrix.\n",
        "\n",
        "Same... but different.\n",
        "\n",
        "The details of the math aren't very crucial, since `Scikit-Learn` will handle all of the computations for us, we just have to remember that when decomposing sparse feature vectors or features that represent counts, it is better to use `SVD` instead of `PCA`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Latent Semantic Analysis\n",
        "\n",
        "Using `TruncatedSVD` on a feature vector of tf-idf values is so common that it has its own name, _Latent Semantic Analysis_, and like `PCA` or clustering, we can use it to uncover some hidden patterns in our data.\n",
        "\n",
        "We'll use it in a slightly different manner than how we used `PCA` to reduce our data before modeling.\n",
        "\n",
        "Like `PCA`, the components of our `TruncatedSVD` decomposition represent new axes for our transformed data, and are linear combinations of the original features in our dataset.\n",
        "\n",
        "Unlike the data we looked at with `PCA`, our feature space here has so many dimensions, that just looking at the top features that contribute to each of our components can give us an idea of the topics in our dataset.\n",
        "\n",
        "This isn't always possible with non-sparse datasets because when every row of a dataset has a value for every feature, and we have few features, a lot of the `PCA` components might end up having the same contributing features, but with different weights.\n",
        "\n",
        "For example, if we consider the diamond dataset, maybe the components from our `PCA` are all mostly made up of different combinations of `length`, `width` and `height`.\n",
        "\n",
        "With a dataset of sparse word count features, maybe our first component is mostly made up of a combination of the words `car` and `bottle`, the second component is mostly a combination of the words `flower` and `water`, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reload Dataset\n",
        "\n",
        "Let's start afresh: let's reload our dataset and run `tf-idf` vectorization.\n",
        "\n",
        "Like with clustering, we won't worry about separating our dataset into train/test subsets since this is mostly exploratory data analysis and we're not so much interested in the predictive capabilities of our models right now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_df = pd.read_csv(\"./data/text/movie_reviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mVec = TfidfVectorizer(stop_words=\"english\", min_df=5, max_df=0.9, max_features=50_000, ngram_range=(1, 2))\n",
        "\n",
        "reviews_vct = mVec.fit_transform(reviews_df[\"review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = mVec.get_feature_names_out()\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decompose\n",
        "\n",
        "We'll decompose our dataset into $10$ components.\n",
        "\n",
        "This means that we'll transform our original $25\\text{,}000 \\times 50\\text{,}000$ sparse tf-idf document matrix into a dense $25\\text{,}000 \\times 10$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svd = TruncatedSVD(n_components=10, random_state=1010)\n",
        "reviews_svd = svd.fit_transform(reviews_vct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are the first few rows of our transformed dataset. Like with `PCA`, the original meaning of our columns is gone and each of these $10$ columns is a linear combination of the original $50\\text{,}000$ features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_svd[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Topic Extraction\n",
        "\n",
        "Unlike `PCA`, the $\\text{top-}8$ features in each of these components should give us an idea of the kinds of documents/records we have in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_top_words(svd.components_, vocab, 7)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "Just like with clustering above, further refinement of the dataset would be needed in order to get very separated and unique topics, but the lists above do show trends in the content of the reviews. We can see certain movie genres and even some indication of the sentiment of the reviews.\n",
        "\n",
        "A possible next step would further filter out the list of allowed words in our initial tokenization and remove certain common words like `movie`, `movies`, `film`, `good`, `bad`, `like`, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification for other dataset.\n",
        "\n",
        "Now that we know all of the tricks of working with text data, let's look at more significant text classification problems.\n",
        "\n",
        "The datasets [HERE](https://github.com/PSAM-5020-2025S-A/5020-utils/tree/refs/heads/main/datasets/text/amazon_reviews) have review information for different categories of amazon products.\n",
        "\n",
        "Books is the largest of the datasets, so we can start with that one.\n",
        "\n",
        "The dataset not only has the text of each review, and some irrelevant information about the reviewer, but also includes a numerical rating of the product. These ratings are whole numbers between $1$ and $5$, which means we're looking at a multi-class classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's Load\n",
        "\n",
        "Let's download, load and take a look at our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -qO- https://github.com/PSAM-5020-2025S-A/5020-utils/raw/refs/heads/main/datasets/text/amazon_reviews/books.tar.gz | tar xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_full_df = pd.read_csv(\"./data/text/amazon_reviews/books.csv\")\n",
        "reviews_full_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a pretty big dataset with $220\\text{,}000$ rows of book reviews.\n",
        "\n",
        "Let's take a closer look at our output label, the `rating` column, to see how its values are distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_full_df[\"rating\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ«¤\n",
        "\n",
        "That's pretty uneven. A model could just guess $5$ all the time and be correct $60\\%$ of the time.\n",
        "\n",
        "When training a classifier we want to keep the values of our output label balanced in order to avoid any kind of artificial biasing of the model during training.\n",
        "\n",
        "Let's re-balance the dataset. We'll do this by storing the number of reviews that have the least common rating and then getting an equal number of random ratings for each of the other possible rating values.\n",
        "\n",
        "We have the ability of grouping our `DataFrame` rows by one of the columns and then sampling an equal number of reviews from each of these groups. The code for doing this is concise, but not very intuitive, requiring some extraneous parameters and function calls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_count = reviews_full_df[\"rating\"].value_counts().min()\n",
        "\n",
        "def sample_min(df):\n",
        "  return df.sample(min_count, random_state=10010)\n",
        "\n",
        "rg = reviews_full_df.groupby(\"rating\")\n",
        "reviews_balanced_df = rg[reviews_full_df.columns].apply(sample_min).reset_index(drop=True)\n",
        "\n",
        "reviews_balanced_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We should have a balanced dataset now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_balanced_df[\"rating\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But, there is one other thing we should check and fix before we start separating our input features and labels.\n",
        "\n",
        "We should check if we have any reviews that don't have values in the `review_text` column.\n",
        "\n",
        "We do this by using the `isna()` function to detect any empty/null values, and then getting the total count of these _na_ values per column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_balanced_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There seems to be some missing `reviewer`, `date` and `title` values, but all of our reviews have a `review_text`.\n",
        "\n",
        "Let's start separating our input and output features from the full dataset.\n",
        "\n",
        "We'll create a separate `DataFrame` that will basically have the `review_text` and `rating` values.\n",
        "\n",
        "We just have to make sure `rating` is represented as a whole number (`int`) so we can use it as the class labels in a `MultinomialNB` classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews_df = pd.DataFrame(reviews_balanced_df[\"rating\"].astype(int))\n",
        "reviews_df[\"review\"] = reviews_balanced_df[\"review_text\"]\n",
        "reviews_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classify !\n",
        "\n",
        "Ok. The data is ready, we just have to:\n",
        "- Split the data into train/test datasets\n",
        "- Vectorize the text column into count or tf-idf features\n",
        "- Train a classifier\n",
        "- Look at confusion matrices and evaluate the classifier\n",
        "- Adjust parameters in the vectorizer or classifier, maybe try n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO:\n",
        "  # T/T Split\n",
        "  # Vectorize\n",
        "  # Classify\n",
        "  # Evaluate\n",
        "  # Try n-grams"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
